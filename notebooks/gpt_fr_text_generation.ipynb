{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-fr-text-generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1xvjxEGvCj1"
      },
      "source": [
        "**Copyright 2021 Antoine SIMOULIN.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5-pmIXxvClt"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YII_pV8IvCnp"
      },
      "source": [
        "# Using GPT-fr ðŸ‡«ðŸ‡·\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" alt=\"GPT-fr logo\" width=\"200\">\n",
        "\n",
        "**GPT-fr** is a French GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHdsOOMUWdR"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Tokenizers. You may also change the hardware to **GPU** since all computation will be much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baX6sAXpUM7F"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install tokenizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-7JKfvWf2H"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NfaAVyUfgA"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aby-xUyuU7wW",
        "outputId": "5cc43ef5-83d7-44f7-eb75-0668a0e1382b"
      },
      "source": [
        "# Check GPU is available and libraries version\n",
        "print('Pytorch version ...............{}'.format(torch.__version__))\n",
        "print('Transformers version ..........{}'.format(transformers.__version__))\n",
        "print('GPU available .................{}'.format('\\u2705' if torch.cuda.device_count() > 0 else '\\u274c'))\n",
        "print('Available devices .............{}'.format(torch.cuda.device_count()))\n",
        "print('Active CUDA Device: ...........{}'.format(torch.cuda.current_device()))\n",
        "print('Current cuda device: ..........{}'.format(torch.cuda.current_device()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version ...............1.9.0+cu102\n",
            "Transformers version ..........4.9.0.dev0\n",
            "GPU available .................âœ…\n",
            "Available devices .............1\n",
            "Active CUDA Device: ...........0\n",
            "Current cuda device: ..........0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQGh4K0mWhxV"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvf343SIanMh"
      },
      "source": [
        "# Query GPU memory used before loading the model.\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "memory_used_s = !nvidia-smi --query-gpu=memory.used --format=csv | grep ' MiB'\n",
        "memory_used_s = int(memory_used_s[0][:-4])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hZ9fOzQWF21",
        "outputId": "787d1b75-50f2-4df7-ea11-f1b88cbf5481"
      },
      "source": [
        "# Load pretrained model and tokenizer.\n",
        "# The model will be downloaded from HuggingFace hub and cached.\n",
        "# It may take ~5 minutes for the first excecution.\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\").to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfUWEKdZatq4",
        "outputId": "8c69a4a9-95c1-4329-e375-59d097dd8f02"
      },
      "source": [
        "# Query GPU memory used after loading the model.\n",
        "memory_used_e = !nvidia-smi --query-gpu=memory.used --format=csv | grep ' MiB'\n",
        "memory_used_e = int(memory_used_e[0][:-4])\n",
        "print(\"Model loaded in GPU memory and uses {:.2f} Go GPU RAM.\".format(float(memory_used_e - memory_used_s)/1024))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded in GPU memory and uses **4.82** Go GPU RAM.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-E-1Cs1WLt2",
        "outputId": "05c18a61-f70c-4d6e-b78d-45419f123f57"
      },
      "source": [
        "# Check number of parameters.\n",
        "print(\"Model has {:,} parameters.\".format(model.num_parameters()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has 1,016,841,728 parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxYZosg0bg0H"
      },
      "source": [
        "# Set model in eval mode (do not apply dropout)\n",
        "model = model.eval()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FYIovXhe0jQ"
      },
      "source": [
        "## Generation parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5JEsIcy-b8k4"
      },
      "source": [
        "#@markdown Options for the `model.generate` method. c.f. documentationn <a href=\"https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate\" target=\"_blank\">here</a>.\n",
        "\n",
        "max_length = 200  #@param {type: \"slider\", min: 100, max: 1024}\n",
        "do_sample = True  #@param {type: \"boolean\"}\n",
        "top_k = 50  #@param {type: \"number\"}\n",
        "top_p = 0.95  #@param {type: \"number\"}\n",
        "num_return_sequences = 1    #@param {type: \"number\"}\n",
        "#@markdown ---\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "r7CRtOORu-9b",
        "outputId": "c3b9a746-e8b4-419a-be6c-010cb29bf5c8"
      },
      "source": [
        "# Generate a sample of text\n",
        "# This should takes a few seconds\n",
        "input_sentence = \"Longtemps je me suis couchÃ© de bonne heure.\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
        "\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=max_length, \n",
        "    do_sample=do_sample,   \n",
        "    top_k=top_k, \n",
        "    top_p=top_p, \n",
        "    num_return_sequences=num_return_sequences\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "tokenizer.decode(beam_outputs[0], skip_special_tokens=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Longtemps je me suis couchÃ© de bonne heure. Une fois de temps en temps je me levais tout simplement. Puis je me couchais et lâ€™aurore me frappait le front sans un bruit. Je me couchais sur le cÃ´tÃ© et jâ€™allais, Ã  grands pas, de lâ€™un Ã  lâ€™autre bout du cimetiÃ¨re. Je nâ€™avais pas de lampe, et, sous lâ€™ombre des arbres qui, Ã  prÃ©sent, se drapent de leurs feuilles mortes, je voyais passer devant moi la procession des Ã¢mes que les rues et la ville avaient faites entrer dans lâ€™oubli. De temps en temps, je mâ€™asseyais sur un banc, dans une attitude de mÃ©ditation silencieuse. Les gens de mon village Ã©taient dans lâ€™impossibilitÃ© de se dire combien ils avaient Ã©tÃ© bons pour moi, car ils avaient toujours eu lâ€™habitude de mâ€™entendre louer leur piÃ©tÃ©, comme ils lâ€™avaient fait pour moi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inivyWGFYg_n"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}