{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gpt-fr-evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1xvjxEGvCj1"
      },
      "source": [
        "**Copyright 2021 Antoine SIMOULIN.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5-pmIXxvClt"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YII_pV8IvCnp"
      },
      "source": [
        "# Evaluate GPT-fr ðŸ‡«ðŸ‡· on FLUE\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" alt=\"GPT-fr logo\" width=\"200\">\n",
        "\n",
        "**GPT-fr** is a French GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en).\n",
        "\n",
        "In this notebook, we provide the minimal script to evaluate the model on the FLUE benchmark ([Le et al., 2020a](#le-2020-en), [2020b](#le-2020-fr)). FLUE aims to better compare and evaluate NLP models for French."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHdsOOMUWdR"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Tokenizers. We also provice some scripts to download the data and fine-tune the model. The scripts are based on the one provided with [FLUE benchmark](https://github.com/getalp/Flaubert)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baX6sAXpUM7F"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install tokenizers\n",
        "!pip install datasets\n",
        "!test -f download_flue_data.sh || wget https://github.com/AntoineSimoulin/gpt-fr/tree/main/scripts/download_flue_data.sh .\n",
        "!test -f run_flue.py || wget https://github.com/AntoineSimoulin/gpt-fr/tree/main/scripts/run_flue.py .\n",
        "!test -f run_flue.py || wget https://github.com/AntoineSimoulin/gpt-fr/tree/main/scripts/spinner.sh .\n",
        "!chmod +x ./download_flue_data.sh\n",
        "!chmod +x ./spinner.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-7JKfvWf2H"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NfaAVyUfgA"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aby-xUyuU7wW",
        "outputId": "f9a1aed7-e403-4f88-be6f-608c7b008e98"
      },
      "source": [
        "# Check GPU is available and libraries version\n",
        "print('Pytorch version ...............{}'.format(torch.__version__))\n",
        "print('Transformers version ..........{}'.format(transformers.__version__))\n",
        "print('GPU available .................{}'.format('\\u2705' if torch.cuda.device_count() > 0 else '\\u274c'))\n",
        "print('Available devices .............{}'.format(torch.cuda.device_count()))\n",
        "print('Active CUDA Device: ...........{}'.format(torch.cuda.current_device()))\n",
        "print('Current cuda device: ..........{}'.format(torch.cuda.current_device()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pytorch version ...............1.9.0+cu102\n",
            "Transformers version ..........4.11.0.dev0\n",
            "GPU available .................âœ…\n",
            "Available devices .............1\n",
            "Active CUDA Device: ...........0\n",
            "Current cuda device: ..........0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vySbbNVFou5G"
      },
      "source": [
        "## Download and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE97AtRJnnVa"
      },
      "source": [
        "FLUE includes 6 tasks with various level of difficulty, degree of formality, and amount of training samples:\n",
        "* The Cross Lingual Sentiment (**CLS**) task is a sentiment classification on Amazon reviews. Each subtask (books, dvd, music) is a bonary classification task (positive/negative).\n",
        "* The Cross-lingual Adversarial Dataset for Paraphrase Identification (**PAWSX**) is a paraphrase identification task. The goal is to predict whether the sentences in these pairs are semantically equivalent or not.\n",
        "* The Cross-lingual NLI (**XNLI**) is a natural language inference task given a premise (p) and an hypothese (h), the goal is to determine whether p entails, contradicts or neither entails nor contradicts h.\n",
        "* The **Parsing and Part-of-Speech Tagging** task aims to infer constituency and dependency syntactic trees and part-of-speech tags.\n",
        "* The Word Sense Disambiguation (**WSD**) is a classification task\n",
        "which aims to predict the sense of words in a given context according to a specific sense inventory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8q_aMinajdpI"
      },
      "source": [
        "TASK = 'CLS-Books' #@param [\"CLS-Books\", \"CLS-DVD\", \"CLS-Music\", \"PAWSX\", \"XNLI\", \"Parsing-Dep\", \"Parsing-Const\", \"WSD-Verb\", \"WSD-Nouns\"]\n",
        "TASK_NAME = TASK.lower().split('-')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0VlgWt1quJA",
        "outputId": "9b05af94-ca3b-414f-8732-7a1a0561ee07"
      },
      "source": [
        "# We download all FLUE data. If you want to download all data, please don't use the flag `-t $TASK`\n",
        "# With `TASK` in \"CLS-Books\" \"CLS-DVD\" \"CLS-Music\" \"PAWSX\" \"XNLI\" \n",
        "# \"Parsing-Dep\" \"Parsing-Const\" \"WSD-Verb\" \"WSD-Nouns\".\n",
        "# The Parsing data are under licences which require to create a account \n",
        " #and need therefore to be manually downloaded.\n",
        "# Please report to https://dokufarm.phil.hhu.de/spmrl2014/ for instructions\n",
        "\n",
        "!test -d ./flue_data || mkdir ./flue_data\n",
        "!./download_flue_data.sh -d ./flue_data -t $TASK"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mâ£½\u001b[m Downloading CLS\n",
            "\u001b[34m\u001b[1mâ£»\u001b[m Preprocessing CLS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FYIovXhe0jQ"
      },
      "source": [
        "## Evaluate on FLUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Vk1969yekZ7-"
      },
      "source": [
        "MODEL = 'asi/gpt-fr-cased-small' #@param {type:\"string\"}\n",
        "MAX_SEQ_LENGTH = 256 #@param {type:\"integer\"}\n",
        "#@markdown batch size and learning rate should be separated with \"/\" for cross validation parameter search.\n",
        "BATCH_SIZES = 8 #@param {type:\"string\"}\n",
        "LEARNING_RATES = 5e-5/3e-5/2e-5/5e-6/1e-6 #@param {type:\"string\"}\n",
        "NUM_TRAIN_EPOCHS = 4 #@param {type:\"integer\"}\n",
        "#@markdown For the CLS task, the train set size is limited. Standard variation might me high and random seed search might impact results. \n",
        "N_SEEDS = 5 #@param {type:\"integer\"}\n",
        "#@markdown If batch size do not fit into device memory, it is possible to adjust the accumulation step. Final batch size will be equals to `GRAD_ACCUMULATION_STEPS * BATCH_SIZE`.\n",
        "GRAD_ACCUMULATION_STEPS = 1 #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "inivyWGFYg_n",
        "outputId": "bb6d1b49-b940-47f3-ed88-daad4e0977a6"
      },
      "source": [
        "!python run_flue.py \\\n",
        "    --train_file /content/flue_data/$TASK/train.tsv \\\n",
        "    --validation_file /content/flue_data/$TASK/dev.tsv \\\n",
        "    --predict_file /content/flue_data/$TASK/test.tsv \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --tokenizer_name $MODEL \\\n",
        "    --output_dir /content/flue_data/$TASK \\\n",
        "    --max_seq_length $MAX_SEQ_LENGTH \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --learning_rates 5e-6 \\\n",
        "    --batch_sizes $BATCH_SIZES \\\n",
        "    --gradient_accumulation_steps $GRAD_ACCUMULATION_STEPS \\\n",
        "    --num_train_epochs $NUM_TRAIN_EPOCHS \\\n",
        "    --n_seeds $N_SEEDS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09/06/2021 16:13:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/06/2021 16:13:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/flue_data/CLS-Books/runs/Sep06_16-13-20_9b1263df06e8,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4.0,\n",
            "output_dir=/content/flue_data/CLS-Books,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=CLS-Books,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/flue_data/CLS-Books,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "\n",
            "start hyper-parameters search with : lr: 5e-06 and batch_size: 8 without seed 0\n",
            "09/06/2021 16:13:20 - WARNING - datasets.builder -   Using custom data configuration default-665dcdfc5830c464\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "run_flue.py:272: FutureWarning: cast_ is deprecated and will be removed in the next major version of datasets. Use DatasetDict.cast instead.\n",
            "  'label': ClassLabel(num_classes=2),\n",
            "100% 1/1 [00:00<00:00, 180.63ba/s]\n",
            "100% 1/1 [00:00<00:00, 603.41ba/s]\n",
            "100% 1/1 [00:00<00:00, 172.08ba/s]\n",
            "09/06/2021 16:13:20 - INFO - filelock -   Lock 140647921587088 acquired on /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee.lock\n",
            "https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp73b00rd3\n",
            "Downloading: 100% 538/538 [00:00<00:00, 655kB/s]\n",
            "storing https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "creating metadata file for /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "09/06/2021 16:13:21 - INFO - filelock -   Lock 140647921587088 released on /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee.lock\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "09/06/2021 16:13:21 - INFO - filelock -   Lock 140647921316880 acquired on /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41.lock\n",
            "https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp55l5_dry\n",
            "Downloading: 100% 853k/853k [00:00<00:00, 2.75MB/s]\n",
            "storing https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "creating metadata file for /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "09/06/2021 16:13:22 - INFO - filelock -   Lock 140647921316880 released on /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41.lock\n",
            "09/06/2021 16:13:22 - INFO - filelock -   Lock 140647921586512 acquired on /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978.lock\n",
            "https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkpbe1ugn\n",
            "Downloading: 100% 513k/513k [00:00<00:00, 1.65MB/s]\n",
            "storing https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "creating metadata file for /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "09/06/2021 16:13:23 - INFO - filelock -   Lock 140647921586512 released on /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978.lock\n",
            "09/06/2021 16:13:23 - INFO - filelock -   Lock 140647921545424 acquired on /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe.lock\n",
            "https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfultiao6\n",
            "Downloading: 100% 121/121 [00:00<00:00, 162kB/s]\n",
            "storing https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "creating metadata file for /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "09/06/2021 16:13:23 - INFO - filelock -   Lock 140647921545424 released on /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe.lock\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "Assigning </s> to the eos_token key of the tokenizer\n",
            "Assigning <s> to the bos_token key of the tokenizer\n",
            "Assigning <unk> to the unk_token key of the tokenizer\n",
            "Assigning <pad> to the pad_token key of the tokenizer\n",
            "Assigning <mask> to the mask_token key of the tokenizer\n",
            "09/06/2021 16:13:25 - INFO - filelock -   Lock 140647921317968 acquired on /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f.lock\n",
            "https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplk6b8t04\n",
            "Downloading: 100% 510M/510M [00:14<00:00, 36.2MB/s]\n",
            "storing https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "creating metadata file for /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "09/06/2021 16:13:39 - INFO - filelock -   Lock 140647921317968 released on /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f.lock\n",
            "loading weights file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "Some weights of the model checkpoint at asi/gpt-fr-cased-small were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at asi/gpt-fr-cased-small and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 2/2 [00:01<00:00,  1.06ba/s]\n",
            "100% 1/1 [00:00<00:00,  3.07ba/s]\n",
            "100% 2/2 [00:01<00:00,  1.24ba/s]\n",
            "09/06/2021 16:13:48 - INFO - __main__ -   Sample 76 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2874, 3686, 214, 258, 22661, 426, 10948, 272, 13166, 262, 17116, 4791, 307, 260, 35174, 16, 361, 371, 7700, 322, 47139, 748, 389, 528, 219, 357, 214, 5024, 214, 545, 307, 376, 2534, 301, 214, 7945, 748, 32110, 294, 260, 1048, 11081, 748, 1035, 281, 11, 2501, 10140, 249, 3160, 214, 1100, 1356, 18128, 16, 281, 11, 11698, 42699, 810, 628, 14566, 307, 260, 18275, 23254, 748, 21107, 40219, 719, 219, 482, 1224, 1686, 16, 421, 371, 2220, 2373, 6245, 244, 376, 2534, 16, 361, 371, 1133, 322, 3087, 207, 11, 408, 5540, 207, 11, 15398, 294, 1849, 608, 262, 33528, 5124, 16, 27594, 249, 24945, 214, 28536, 450, 608, 310, 44222, 748, 455, 226, 11, 18918, 244, 262, 7945, 249, 487, 357, 20992, 16, 281, 11, 222, 210, 11, 6011, 214, 1247, 239, 12231, 207, 11, 408, 34421, 207, 11, 2693, 6546, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"Les livres de Lignac sont placÃ©s au rayon des Grands chefs dans les librairies, il ne faudrait pas abuser ! Il y a plus de photos de lui dans ce livre que de recettes ! Idem pour les autres volumes ! Si j'avais souhaitÃ© un album de notre cher Cyril, j'aurai dÃ©coupÃ© tous ces portraits dans les magazines People ! Jamie Olliver a fait beaucoup mieux, je ne vois aucun intÃ©rÃªt Ã  ce livre, il ne faut pas sortir d'une Ã©cole d'ingÃ©nieur pour savoir faire des crÃªpes, cuire un pavÃ© de saumon ou faire une purÃ©e ! Je m'attendais Ã  des recettes un peu plus originales, j'ai l'impression de voir le menu d'une cantine d'Ã©cole primaire.\"}.\n",
            "09/06/2021 16:13:48 - INFO - __main__ -   Sample 1465 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [6554, 533, 27028, 249, 2534, 3047, 207, 11, 35378, 244, 18292, 278, 294, 4019, 246, 10730, 260, 295, 41855, 17, 42, 88, 214, 394, 1094, 16, 371, 4443, 322, 504, 960, 16, 14152, 1769, 17, 211, 249, 1130, 18, 1035, 533, 4869, 10788, 376, 8534, 207, 11, 232, 1284, 214, 2140, 21327, 16, 728, 39999, 1347, 12616, 16, 250, 19591, 307, 239, 3723, 262, 32469, 16, 728, 1849, 1593, 4862, 210, 11, 8154, 208, 502, 16, 5633, 17, 92, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"Si vous cherchez un livre simple d'initiation Ã  Scheme pour comprendre et modifier les Script-Fu de Gimp, ne faites pas comme moi, choisissez-en un autre. Si vous voulez Ã©tudier ce langage d'un point de vue thÃ©orique, sans allumer votre ordinateur, en entrant dans le dÃ©tail des algorithmes, sans savoir comment lancer l'interprÃ©teur, allez-y.\"}.\n",
            "09/06/2021 16:13:48 - INFO - __main__ -   Sample 1540 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2285, 1921, 14058, 6984, 272, 5203, 24614, 16, 260, 36952, 360, 33737, 533, 9737, 461, 1074, 18, 367, 4736, 4014, 314, 699, 661, 1181, 264, 330, 1113, 207, 11, 2007, 207, 11, 3443, 5542, 24846, 383, 28412, 18, 666, 323, 1003, 6664, 307, 234, 5881, 262, 5238, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"La situation gÃ©ographique prÃªte au rÃªve oriental, les senteurs dÃ©licates vous ennivrent. Le contexte historique est trÃ¨s interressant sur fond d'histoire d'amour impossible traitÃ©e avec dÃ©licatesse. On se met facilement dans la peau des personnages.\"}.\n",
            "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 800\n",
            "{'loss': 0.4438, 'learning_rate': 1.8750000000000003e-06, 'epoch': 2.5}\n",
            "100% 800/800 [05:53<00:00,  2.32it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 353.4817, 'train_samples_per_second': 18.094, 'train_steps_per_second': 2.263, 'train_loss': 0.3600461959838867, 'epoch': 4.0}\n",
            "100% 800/800 [05:53<00:00,  2.26it/s]\n",
            "Saving model checkpoint to /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8\n",
            "Configuration saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/config.json\n",
            "Model weights saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/pytorch_model.bin\n",
            "tokenizer config file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/tokenizer_config.json\n",
            "Special tokens file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/special_tokens_map.json\n",
            "09/06/2021 16:19:49 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 399\n",
            "  Batch size = 8\n",
            "100% 50/50 [00:07<00:00,  7.13it/s]\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1999\n",
            "  Batch size = 8\n",
            "100% 250/250 [00:36<00:00,  6.94it/s]\n",
            "09/06/2021 16:20:33 - INFO - __main__ -   ***** Eval results cls *****\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     eval_loss = 0.4606432616710663\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     eval_accuracy = 0.8621553884711779\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     eval_runtime = 7.398\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     eval_samples_per_second = 53.933\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     eval_steps_per_second = 6.759\n",
            "09/06/2021 16:20:33 - INFO - __main__ -     epoch = 4.0\n",
            "\n",
            "start hyper-parameters search with : lr: 5e-06 and batch_size: 8 without seed 1\n",
            "09/06/2021 16:20:33 - WARNING - datasets.builder -   Using custom data configuration default-665dcdfc5830c464\n",
            "09/06/2021 16:20:33 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "09/06/2021 16:20:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1b016ff4da8063ca.arrow\n",
            "09/06/2021 16:20:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-90f985e022facb9b.arrow\n",
            "09/06/2021 16:20:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-3111a60baca4fa5e.arrow\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "Assigning </s> to the eos_token key of the tokenizer\n",
            "Assigning <s> to the bos_token key of the tokenizer\n",
            "Assigning <unk> to the unk_token key of the tokenizer\n",
            "Assigning <pad> to the pad_token key of the tokenizer\n",
            "Assigning <mask> to the mask_token key of the tokenizer\n",
            "loading weights file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "Some weights of the model checkpoint at asi/gpt-fr-cased-small were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at asi/gpt-fr-cased-small and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "09/06/2021 16:20:38 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-01eca7454d23239d.arrow\n",
            "100% 1/1 [00:00<00:00,  1.90ba/s]\n",
            "100% 2/2 [00:01<00:00,  1.04ba/s]\n",
            "09/06/2021 16:20:42 - INFO - __main__ -   Sample 1309 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [11067, 19027, 214, 2808, 35668, 250, 27911, 503, 3114, 294, 239, 871, 27906, 18, 258, 11, 23161, 314, 6892, 6502, 16, 239, 19532, 1373, 214, 1865, 275, 1179, 244, 234, 532, 599, 301, 3431, 25183, 214, 525, 450, 214, 2013, 234, 46916, 275, 3199, 455, 3799, 4332, 18, 732, 319, 314, 9462, 16, 217, 11, 263, 214, 4019, 210, 11, 2453, 214, 376, 8421, 2663, 686, 35224, 236, 11, 263, 17, 267, 319, 219, 10824, 249, 1462, 728, 3114, 244, 4938, 3242, 249, 6373, 246, 664, 6373, 35224, 310, 4042, 25004, 88, 7290, 748, 367, 357, 41088, 307, 503, 3114, 314, 214, 323, 660, 301, 3431, 207, 11, 443, 546, 30022, 249, 9580, 18, 48423, 515, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"Impossible de rester insensible en lisant cette histoire pour le moins inquiÃ©tante. L'intrigue est parfaitement menÃ©e, le suspense reste de mise du dÃ©but Ã  la fin bien que chacun connaisse de prÃ©s ou de loin la schizophrÃ©nie du Dr Jekyll. Ce qui est intÃ©ressant, c'est de comprendre l'origine de ce dÃ©doublement : qu'est-ce qui a amenÃ© un homme sans histoire Ã  vouloir devenir un Autre et quel Autre : une vÃ©ritable monstruositÃ© ! Le plus troublant dans cette histoire est de se dire que chacun d'entre nous renferme un Mr. Hyde...\"}.\n",
            "09/06/2021 16:20:42 - INFO - __main__ -   Sample 228 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [6554, 210, 11, 3136, 35859, 314, 13591, 16, 421, 243, 11, 3408, 322, 1887, 234, 2099, 723, 210, 11, 2532, 6778, 376, 2534, 30, 234, 532, 314, 33868, 16, 210, 11, 11316, 262, 5238, 594, 18, 210, 11, 44017, 47260, 214, 503, 1787, 314, 980, 808, 38796, 18, 421, 16918, 214, 239, 4168, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"Si l'idÃ©e originelle est excellente, je n'aime pas vraiment la faÃ§on dont l'auteur traite ce livre: la fin est dÃ©cevante, l' Ã©volution des personnages aussi. l' Analyse sociologique de cette sociÃ©tÃ© est toute fois pertinente. je conseille de le lire.\"}.\n",
            "09/06/2021 16:20:42 - INFO - __main__ -   Sample 51 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [50, 11, 263, 322, 16045, 17, 45810, 319, 1561, 18, 367, 998, 17, 32831, 16, 210, 11, 11068, 32085, 16, 503, 2807, 214, 13630, 3308, 8701, 11547, 16, 371, 234, 8733, 322, 319, 1561, 18, 704, 3455, 20208, 371, 234, 8733, 322, 18, 1566, 3560, 3549, 339, 32150, 6287, 13, 314, 249, 968, 1501, 529, 44699, 16, 663, 1328, 260, 5214, 17304, 244, 8296, 16, 663, 260, 9045, 214, 9833, 1907, 1506, 458, 16, 13503, 16, 244, 883, 260, 1064, 2487, 6287, 16, 319, 2168, 833, 249, 487, 239, 1373, 16, 210, 11, 2517, 314, 9604, 551, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"N'est pas anglo-saxon qui veut. Le non-sens, l'humour dÃ©calÃ©, cette marque de fabrique so british, ne la maÃ®trise pas qui veut. Et Martin Page ne la maÃ®trise pas. Son court roman (120 pages) est un long pensum laborieux, oÃ¹ toutes les idÃ©es tombent Ã  plat, oÃ¹ les essais de poÃ©sie font flop, bref, Ã  part les quelques derniÃ¨res pages, qui sauvent un peu le reste, l'ensemble est oubliable.\"}.\n",
            "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 800\n",
            "{'loss': 0.3785, 'learning_rate': 1.8750000000000003e-06, 'epoch': 2.5}\n",
            "100% 800/800 [05:58<00:00,  2.31it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 358.2236, 'train_samples_per_second': 17.855, 'train_steps_per_second': 2.233, 'train_loss': 0.3178840732574463, 'epoch': 4.0}\n",
            "100% 800/800 [05:58<00:00,  2.23it/s]\n",
            "Saving model checkpoint to /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8\n",
            "Configuration saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/config.json\n",
            "Model weights saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/pytorch_model.bin\n",
            "tokenizer config file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/tokenizer_config.json\n",
            "Special tokens file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/special_tokens_map.json\n",
            "09/06/2021 16:26:42 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 399\n",
            "  Batch size = 8\n",
            "100% 50/50 [00:07<00:00,  7.08it/s]\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1999\n",
            "  Batch size = 8\n",
            "100% 250/250 [00:35<00:00,  6.96it/s]\n",
            "09/06/2021 16:27:25 - INFO - __main__ -   ***** Eval results cls *****\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     eval_loss = 0.5335955619812012\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     eval_accuracy = 0.8822055137844611\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     eval_runtime = 7.1893\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     eval_samples_per_second = 55.499\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     eval_steps_per_second = 6.955\n",
            "09/06/2021 16:27:25 - INFO - __main__ -     epoch = 4.0\n",
            "\n",
            "start hyper-parameters search with : lr: 5e-06 and batch_size: 8 without seed 2\n",
            "09/06/2021 16:27:26 - WARNING - datasets.builder -   Using custom data configuration default-665dcdfc5830c464\n",
            "09/06/2021 16:27:26 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "09/06/2021 16:27:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1b016ff4da8063ca.arrow\n",
            "09/06/2021 16:27:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-90f985e022facb9b.arrow\n",
            "09/06/2021 16:27:26 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-3111a60baca4fa5e.arrow\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "Assigning </s> to the eos_token key of the tokenizer\n",
            "Assigning <s> to the bos_token key of the tokenizer\n",
            "Assigning <unk> to the unk_token key of the tokenizer\n",
            "Assigning <pad> to the pad_token key of the tokenizer\n",
            "Assigning <mask> to the mask_token key of the tokenizer\n",
            "loading weights file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "Some weights of the model checkpoint at asi/gpt-fr-cased-small were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at asi/gpt-fr-cased-small and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "09/06/2021 16:27:30 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-01eca7454d23239d.arrow\n",
            "09/06/2021 16:27:31 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-97a7a5a5a66c441e.arrow\n",
            "100% 2/2 [00:02<00:00,  1.05s/ba]\n",
            "09/06/2021 16:27:34 - INFO - __main__ -   Sample 1309 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [11067, 19027, 214, 2808, 35668, 250, 27911, 503, 3114, 294, 239, 871, 27906, 18, 258, 11, 23161, 314, 6892, 6502, 16, 239, 19532, 1373, 214, 1865, 275, 1179, 244, 234, 532, 599, 301, 3431, 25183, 214, 525, 450, 214, 2013, 234, 46916, 275, 3199, 455, 3799, 4332, 18, 732, 319, 314, 9462, 16, 217, 11, 263, 214, 4019, 210, 11, 2453, 214, 376, 8421, 2663, 686, 35224, 236, 11, 263, 17, 267, 319, 219, 10824, 249, 1462, 728, 3114, 244, 4938, 3242, 249, 6373, 246, 664, 6373, 35224, 310, 4042, 25004, 88, 7290, 748, 367, 357, 41088, 307, 503, 3114, 314, 214, 323, 660, 301, 3431, 207, 11, 443, 546, 30022, 249, 9580, 18, 48423, 515, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"Impossible de rester insensible en lisant cette histoire pour le moins inquiÃ©tante. L'intrigue est parfaitement menÃ©e, le suspense reste de mise du dÃ©but Ã  la fin bien que chacun connaisse de prÃ©s ou de loin la schizophrÃ©nie du Dr Jekyll. Ce qui est intÃ©ressant, c'est de comprendre l'origine de ce dÃ©doublement : qu'est-ce qui a amenÃ© un homme sans histoire Ã  vouloir devenir un Autre et quel Autre : une vÃ©ritable monstruositÃ© ! Le plus troublant dans cette histoire est de se dire que chacun d'entre nous renferme un Mr. Hyde...\"}.\n",
            "09/06/2021 16:27:34 - INFO - __main__ -   Sample 228 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [6554, 210, 11, 3136, 35859, 314, 13591, 16, 421, 243, 11, 3408, 322, 1887, 234, 2099, 723, 210, 11, 2532, 6778, 376, 2534, 30, 234, 532, 314, 33868, 16, 210, 11, 11316, 262, 5238, 594, 18, 210, 11, 44017, 47260, 214, 503, 1787, 314, 980, 808, 38796, 18, 421, 16918, 214, 239, 4168, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"Si l'idÃ©e originelle est excellente, je n'aime pas vraiment la faÃ§on dont l'auteur traite ce livre: la fin est dÃ©cevante, l' Ã©volution des personnages aussi. l' Analyse sociologique de cette sociÃ©tÃ© est toute fois pertinente. je conseille de le lire.\"}.\n",
            "09/06/2021 16:27:34 - INFO - __main__ -   Sample 51 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [50, 11, 263, 322, 16045, 17, 45810, 319, 1561, 18, 367, 998, 17, 32831, 16, 210, 11, 11068, 32085, 16, 503, 2807, 214, 13630, 3308, 8701, 11547, 16, 371, 234, 8733, 322, 319, 1561, 18, 704, 3455, 20208, 371, 234, 8733, 322, 18, 1566, 3560, 3549, 339, 32150, 6287, 13, 314, 249, 968, 1501, 529, 44699, 16, 663, 1328, 260, 5214, 17304, 244, 8296, 16, 663, 260, 9045, 214, 9833, 1907, 1506, 458, 16, 13503, 16, 244, 883, 260, 1064, 2487, 6287, 16, 319, 2168, 833, 249, 487, 239, 1373, 16, 210, 11, 2517, 314, 9604, 551, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"N'est pas anglo-saxon qui veut. Le non-sens, l'humour dÃ©calÃ©, cette marque de fabrique so british, ne la maÃ®trise pas qui veut. Et Martin Page ne la maÃ®trise pas. Son court roman (120 pages) est un long pensum laborieux, oÃ¹ toutes les idÃ©es tombent Ã  plat, oÃ¹ les essais de poÃ©sie font flop, bref, Ã  part les quelques derniÃ¨res pages, qui sauvent un peu le reste, l'ensemble est oubliable.\"}.\n",
            "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 800\n",
            "{'loss': 0.3785, 'learning_rate': 1.8750000000000003e-06, 'epoch': 2.5}\n",
            "100% 800/800 [05:58<00:00,  2.31it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 358.2534, 'train_samples_per_second': 17.853, 'train_steps_per_second': 2.233, 'train_loss': 0.3178840732574463, 'epoch': 4.0}\n",
            "100% 800/800 [05:58<00:00,  2.23it/s]\n",
            "Saving model checkpoint to /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8\n",
            "Configuration saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/config.json\n",
            "Model weights saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/pytorch_model.bin\n",
            "tokenizer config file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/tokenizer_config.json\n",
            "Special tokens file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/special_tokens_map.json\n",
            "09/06/2021 16:33:34 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 399\n",
            "  Batch size = 8\n",
            "100% 50/50 [00:07<00:00,  7.10it/s]\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1999\n",
            "  Batch size = 8\n",
            "100% 250/250 [00:35<00:00,  6.95it/s]\n",
            "09/06/2021 16:34:18 - INFO - __main__ -   ***** Eval results cls *****\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     eval_loss = 0.5335955619812012\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     eval_accuracy = 0.8822055137844611\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     eval_runtime = 7.178\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     eval_samples_per_second = 55.587\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     eval_steps_per_second = 6.966\n",
            "09/06/2021 16:34:18 - INFO - __main__ -     epoch = 4.0\n",
            "\n",
            "start hyper-parameters search with : lr: 5e-06 and batch_size: 8 without seed 3\n",
            "09/06/2021 16:34:18 - WARNING - datasets.builder -   Using custom data configuration default-665dcdfc5830c464\n",
            "09/06/2021 16:34:18 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "09/06/2021 16:34:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1b016ff4da8063ca.arrow\n",
            "09/06/2021 16:34:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-90f985e022facb9b.arrow\n",
            "09/06/2021 16:34:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-3111a60baca4fa5e.arrow\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "Assigning </s> to the eos_token key of the tokenizer\n",
            "Assigning <s> to the bos_token key of the tokenizer\n",
            "Assigning <unk> to the unk_token key of the tokenizer\n",
            "Assigning <pad> to the pad_token key of the tokenizer\n",
            "Assigning <mask> to the mask_token key of the tokenizer\n",
            "loading weights file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "Some weights of the model checkpoint at asi/gpt-fr-cased-small were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at asi/gpt-fr-cased-small and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "09/06/2021 16:34:23 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-01eca7454d23239d.arrow\n",
            "09/06/2021 16:34:24 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-97a7a5a5a66c441e.arrow\n",
            "09/06/2021 16:34:24 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-7a1a88ef7ad42912.arrow\n",
            "09/06/2021 16:34:24 - INFO - __main__ -   Sample 1309 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [11067, 19027, 214, 2808, 35668, 250, 27911, 503, 3114, 294, 239, 871, 27906, 18, 258, 11, 23161, 314, 6892, 6502, 16, 239, 19532, 1373, 214, 1865, 275, 1179, 244, 234, 532, 599, 301, 3431, 25183, 214, 525, 450, 214, 2013, 234, 46916, 275, 3199, 455, 3799, 4332, 18, 732, 319, 314, 9462, 16, 217, 11, 263, 214, 4019, 210, 11, 2453, 214, 376, 8421, 2663, 686, 35224, 236, 11, 263, 17, 267, 319, 219, 10824, 249, 1462, 728, 3114, 244, 4938, 3242, 249, 6373, 246, 664, 6373, 35224, 310, 4042, 25004, 88, 7290, 748, 367, 357, 41088, 307, 503, 3114, 314, 214, 323, 660, 301, 3431, 207, 11, 443, 546, 30022, 249, 9580, 18, 48423, 515, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"Impossible de rester insensible en lisant cette histoire pour le moins inquiÃ©tante. L'intrigue est parfaitement menÃ©e, le suspense reste de mise du dÃ©but Ã  la fin bien que chacun connaisse de prÃ©s ou de loin la schizophrÃ©nie du Dr Jekyll. Ce qui est intÃ©ressant, c'est de comprendre l'origine de ce dÃ©doublement : qu'est-ce qui a amenÃ© un homme sans histoire Ã  vouloir devenir un Autre et quel Autre : une vÃ©ritable monstruositÃ© ! Le plus troublant dans cette histoire est de se dire que chacun d'entre nous renferme un Mr. Hyde...\"}.\n",
            "09/06/2021 16:34:24 - INFO - __main__ -   Sample 228 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [6554, 210, 11, 3136, 35859, 314, 13591, 16, 421, 243, 11, 3408, 322, 1887, 234, 2099, 723, 210, 11, 2532, 6778, 376, 2534, 30, 234, 532, 314, 33868, 16, 210, 11, 11316, 262, 5238, 594, 18, 210, 11, 44017, 47260, 214, 503, 1787, 314, 980, 808, 38796, 18, 421, 16918, 214, 239, 4168, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"Si l'idÃ©e originelle est excellente, je n'aime pas vraiment la faÃ§on dont l'auteur traite ce livre: la fin est dÃ©cevante, l' Ã©volution des personnages aussi. l' Analyse sociologique de cette sociÃ©tÃ© est toute fois pertinente. je conseille de le lire.\"}.\n",
            "09/06/2021 16:34:24 - INFO - __main__ -   Sample 51 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [50, 11, 263, 322, 16045, 17, 45810, 319, 1561, 18, 367, 998, 17, 32831, 16, 210, 11, 11068, 32085, 16, 503, 2807, 214, 13630, 3308, 8701, 11547, 16, 371, 234, 8733, 322, 319, 1561, 18, 704, 3455, 20208, 371, 234, 8733, 322, 18, 1566, 3560, 3549, 339, 32150, 6287, 13, 314, 249, 968, 1501, 529, 44699, 16, 663, 1328, 260, 5214, 17304, 244, 8296, 16, 663, 260, 9045, 214, 9833, 1907, 1506, 458, 16, 13503, 16, 244, 883, 260, 1064, 2487, 6287, 16, 319, 2168, 833, 249, 487, 239, 1373, 16, 210, 11, 2517, 314, 9604, 551, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"N'est pas anglo-saxon qui veut. Le non-sens, l'humour dÃ©calÃ©, cette marque de fabrique so british, ne la maÃ®trise pas qui veut. Et Martin Page ne la maÃ®trise pas. Son court roman (120 pages) est un long pensum laborieux, oÃ¹ toutes les idÃ©es tombent Ã  plat, oÃ¹ les essais de poÃ©sie font flop, bref, Ã  part les quelques derniÃ¨res pages, qui sauvent un peu le reste, l'ensemble est oubliable.\"}.\n",
            "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 800\n",
            "{'loss': 0.3785, 'learning_rate': 1.8750000000000003e-06, 'epoch': 2.5}\n",
            "100% 800/800 [05:58<00:00,  2.32it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 358.0424, 'train_samples_per_second': 17.864, 'train_steps_per_second': 2.234, 'train_loss': 0.3178840732574463, 'epoch': 4.0}\n",
            "100% 800/800 [05:58<00:00,  2.23it/s]\n",
            "Saving model checkpoint to /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/gpt-fr-cased-small/flue/cls/3_5e-06_8\n",
            "Configuration saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/gpt-fr-cased-small/flue/cls/3_5e-06_8/config.json\n",
            "Model weights saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/gpt-fr-cased-small/flue/cls/3_5e-06_8/pytorch_model.bin\n",
            "tokenizer config file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/gpt-fr-cased-small/flue/cls/3_5e-06_8/tokenizer_config.json\n",
            "Special tokens file saved in /content/flue_data/CLS-Books/gpt-fr-cased-small/flue/cls/0_5e-06_8/gpt-fr-cased-small/flue/cls/1_5e-06_8/gpt-fr-cased-small/flue/cls/2_5e-06_8/gpt-fr-cased-small/flue/cls/3_5e-06_8/special_tokens_map.json\n",
            "09/06/2021 16:40:24 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 399\n",
            "  Batch size = 8\n",
            "100% 50/50 [00:07<00:00,  7.07it/s]\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1999\n",
            "  Batch size = 8\n",
            "100% 250/250 [00:36<00:00,  6.94it/s]\n",
            "09/06/2021 16:41:08 - INFO - __main__ -   ***** Eval results cls *****\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     eval_loss = 0.5335955619812012\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     eval_accuracy = 0.8822055137844611\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     eval_runtime = 7.2051\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     eval_samples_per_second = 55.377\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     eval_steps_per_second = 6.939\n",
            "09/06/2021 16:41:08 - INFO - __main__ -     epoch = 4.0\n",
            "\n",
            "start hyper-parameters search with : lr: 5e-06 and batch_size: 8 without seed 4\n",
            "09/06/2021 16:41:08 - WARNING - datasets.builder -   Using custom data configuration default-665dcdfc5830c464\n",
            "09/06/2021 16:41:08 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "09/06/2021 16:41:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1b016ff4da8063ca.arrow\n",
            "09/06/2021 16:41:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-90f985e022facb9b.arrow\n",
            "09/06/2021 16:41:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-3111a60baca4fa5e.arrow\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/97d73519e1786bd36d6dab4f2240e77dc8b19cc8535b19f0eb0cc5863d9b6c81.b85636952522fed3a170e2e21a847e912c3a878dedc23912f85546cfa1227f41\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/025c7f852122770d236ec27f3dd32ac9e1f40679c14a8c4bea80600b7ba0add6.e53643bb177d00116553f4d730afde4d2f8f45c1447a76aa963ba9a0a1b73978\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8bb3968cc09271da6a1adedd33275c0b14b45d7fc81d5ccb6920d4940075b7fe.0f671f161b2dbdaa3a65d346190cb627aac7c67d9c6468ea6a435d7762d446fe\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12889df88e5263ed62e20128cbb3bebe828aedfd8480954c73d86108d31431a5.97fc21b74d14a9facfb92cee58e1cc1b6abc510049602d7ae0620e0d4f7eacee\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "Assigning </s> to the eos_token key of the tokenizer\n",
            "Assigning <s> to the bos_token key of the tokenizer\n",
            "Assigning <unk> to the unk_token key of the tokenizer\n",
            "Assigning <pad> to the pad_token key of the tokenizer\n",
            "Assigning <mask> to the mask_token key of the tokenizer\n",
            "loading weights file https://huggingface.co/asi/gpt-fr-cased-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/16bd2bed8e0f184b6d447c39c2c4bf64135b888c51056ccb56ae0f0bfd9c12a6.661b4443ec5caefcf86cfc76c9bd77815c311c96907d9e2b32129a2bf079cf2f\n",
            "Some weights of the model checkpoint at asi/gpt-fr-cased-small were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at asi/gpt-fr-cased-small and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "09/06/2021 16:41:13 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-01eca7454d23239d.arrow\n",
            "09/06/2021 16:41:14 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-97a7a5a5a66c441e.arrow\n",
            "09/06/2021 16:41:15 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-665dcdfc5830c464/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-7a1a88ef7ad42912.arrow\n",
            "09/06/2021 16:41:15 - INFO - __main__ -   Sample 1309 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [11067, 19027, 214, 2808, 35668, 250, 27911, 503, 3114, 294, 239, 871, 27906, 18, 258, 11, 23161, 314, 6892, 6502, 16, 239, 19532, 1373, 214, 1865, 275, 1179, 244, 234, 532, 599, 301, 3431, 25183, 214, 525, 450, 214, 2013, 234, 46916, 275, 3199, 455, 3799, 4332, 18, 732, 319, 314, 9462, 16, 217, 11, 263, 214, 4019, 210, 11, 2453, 214, 376, 8421, 2663, 686, 35224, 236, 11, 263, 17, 267, 319, 219, 10824, 249, 1462, 728, 3114, 244, 4938, 3242, 249, 6373, 246, 664, 6373, 35224, 310, 4042, 25004, 88, 7290, 748, 367, 357, 41088, 307, 503, 3114, 314, 214, 323, 660, 301, 3431, 207, 11, 443, 546, 30022, 249, 9580, 18, 48423, 515, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': \"Impossible de rester insensible en lisant cette histoire pour le moins inquiÃ©tante. L'intrigue est parfaitement menÃ©e, le suspense reste de mise du dÃ©but Ã  la fin bien que chacun connaisse de prÃ©s ou de loin la schizophrÃ©nie du Dr Jekyll. Ce qui est intÃ©ressant, c'est de comprendre l'origine de ce dÃ©doublement : qu'est-ce qui a amenÃ© un homme sans histoire Ã  vouloir devenir un Autre et quel Autre : une vÃ©ritable monstruositÃ© ! Le plus troublant dans cette histoire est de se dire que chacun d'entre nous renferme un Mr. Hyde...\"}.\n",
            "09/06/2021 16:41:15 - INFO - __main__ -   Sample 228 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [6554, 210, 11, 3136, 35859, 314, 13591, 16, 421, 243, 11, 3408, 322, 1887, 234, 2099, 723, 210, 11, 2532, 6778, 376, 2534, 30, 234, 532, 314, 33868, 16, 210, 11, 11316, 262, 5238, 594, 18, 210, 11, 44017, 47260, 214, 503, 1787, 314, 980, 808, 38796, 18, 421, 16918, 214, 239, 4168, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"Si l'idÃ©e originelle est excellente, je n'aime pas vraiment la faÃ§on dont l'auteur traite ce livre: la fin est dÃ©cevante, l' Ã©volution des personnages aussi. l' Analyse sociologique de cette sociÃ©tÃ© est toute fois pertinente. je conseille de le lire.\"}.\n",
            "09/06/2021 16:41:15 - INFO - __main__ -   Sample 51 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [50, 11, 263, 322, 16045, 17, 45810, 319, 1561, 18, 367, 998, 17, 32831, 16, 210, 11, 11068, 32085, 16, 503, 2807, 214, 13630, 3308, 8701, 11547, 16, 371, 234, 8733, 322, 319, 1561, 18, 704, 3455, 20208, 371, 234, 8733, 322, 18, 1566, 3560, 3549, 339, 32150, 6287, 13, 314, 249, 968, 1501, 529, 44699, 16, 663, 1328, 260, 5214, 17304, 244, 8296, 16, 663, 260, 9045, 214, 9833, 1907, 1506, 458, 16, 13503, 16, 244, 883, 260, 1064, 2487, 6287, 16, 319, 2168, 833, 249, 487, 239, 1373, 16, 210, 11, 2517, 314, 9604, 551, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': \"N'est pas anglo-saxon qui veut. Le non-sens, l'humour dÃ©calÃ©, cette marque de fabrique so british, ne la maÃ®trise pas qui veut. Et Martin Page ne la maÃ®trise pas. Son court roman (120 pages) est un long pensum laborieux, oÃ¹ toutes les idÃ©es tombent Ã  plat, oÃ¹ les essais de poÃ©sie font flop, bref, Ã  part les quelques derniÃ¨res pages, qui sauvent un peu le reste, l'ensemble est oubliable.\"}.\n",
            "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence.\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 800\n",
            " 34% 270/800 [02:01<03:59,  2.21it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvMacM_Fmxhq"
      },
      "source": [
        "## References\n",
        "\n",
        "><div id=\"le-2020-en\">Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab:\n",
        "<a href=\"https://www.aclweb.org/anthology/2020.lrec-1.302/\"> FlauBERT: Unsupervised Language Model Pre-training for French</a>. LREC 2020: 2479-2490</div>\n",
        "\n",
        "><div id=\"le-2020-fr\">Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab:\n",
        "<a href=\"https://www.aclweb.org/anthology/2020.jeptalnrecital-taln.26/\">FlauBERT : des modÃ¨les de langue contextualisÃ©s prÃ©-entraÃ®nÃ©s pour le franÃ§ais</a> (FlauBERT : Unsupervised Language Model Pre-training for French). JEP-TALN-RECITAL (2) 2020: 268-278</div>\n",
        "\n"
      ]
    }
  ]
}