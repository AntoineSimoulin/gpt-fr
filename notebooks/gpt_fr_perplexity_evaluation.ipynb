{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Copyright 2021 Antoine SIMOULIN.**\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating GPT-fr on the Wikitext-fr benchmark ðŸ‡«ðŸ‡·\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" alt=\"GPT-fr logo\" width=\"200\">\n",
    "\n",
    "**GPT-fr** is a French GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers, ðŸ¤— Tokenizers and ðŸ¤— Datasets. You may also change the hardware to **GPU** since all computation will be much faster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install tokenizers\n",
    "!pip install datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check GPU is available and libraries version\n",
    "print('Pytorch version ...............{}'.format(torch.__version__))\n",
    "print('Transformers version ..........{}'.format(transformers.__version__))\n",
    "print('Datasets version ..............{}'.format(datasets.__version__))\n",
    "print('GPU available .................{}'.format('\\u2705' if torch.cuda.device_count() > 0 else '\\u274c'))\n",
    "print('Available devices .............{}'.format(torch.cuda.device_count()))\n",
    "print('Active CUDA Device: ...........{}'.format(torch.cuda.current_device()))\n",
    "print('Current cuda device: ..........{}'.format(torch.cuda.current_device()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# Load pretrained model and tokenizer.\n",
    "# The model will be downloaded from HuggingFace hub and cached.\n",
    "# It may take ~5 minutes for the first excecution.\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set model in eval mode (do not apply dropout)\n",
    "model = model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We concatenate all paragraphs from the dataset and encode them using the tokenizer.\n",
    "\n",
    "dataset = load_dataset(\n",
    "  \"asi/wikitext_fr\",\n",
    "  \"wikitext-72\"\n",
    ")\n",
    "\n",
    "dataset = [l['paragraph'].rstrip() for l in dataset['test']]\n",
    "dataset = [l for l in dataset if l]\n",
    "\n",
    "encodings = tokenizer(' '.join(dataset), return_tensors='pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_length = model.config.n_positions\n",
    "stride = 1024\n",
    "\n",
    "lls = []\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "    trg_len = end_loc - i    # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:,begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:,:-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "    lls.append(log_likelihood)\n",
    "\n",
    "ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "\n",
    "print(\"perplexity on the wikitext test set is {:.2f}\".format(ppl))\n",
    "\n",
    "# perplexity on the wikitext test set is 12.9 with gpt-fr-base\n",
    "# perplexity on the wikitext test set is 109,2 with gpt-fr-small"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}